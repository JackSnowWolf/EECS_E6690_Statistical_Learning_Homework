--- 
title: "EECS E6690 hw1"
author: "Chong Hu ch3467"
date: 'Sep 22, 2019'
output: 
  pdf_document:
    latex_engine: pdflatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\section*{P1}
\begin{enumerate}
    \item[(a)] 
    \begin{align*}
        (n-1) S^2 + n\bar{X}^2  &= \sum_{i=1}^{n} (X_i - \bar{X})^2 + n\bar{X}^2 \\
        &=   \sum_{i=1}^{n} (X_i^2 - 2X_i\bar{X} + \bar{X}^2) + n\bar{X}^2 \\
        &= \sum_{i=1}^{} X_i^2 -2n\bar{X} +n(\bar{X})^2 + n\bar{X}^2 \\
        &= \sum_{i=1}^{} X_i^2
    \end{align*}
    Therefore,
    \begin{equation*}
        \sum_{i=1}^{n} X_i^2 = (n-1) S^2 + n\bar{X}^2 
    \end{equation*}
    \item[(b)]
    According to the question, we have
    \begin{equation*}
       \mathrm{Var}[X_i] =  \mathbb{E}[(X_i-\mathbb{E}[X_i])^2] = \sigma^2 \quad  \forall i = 1,2,3,..,,n
    \end{equation*}
    and we can assume,
    \begin{equation*}
        \mathbb{E}[X_i] = \mu
    \end{equation*}
    hence,
    \begin{equation*}
        \mathrm{Var}[X_i] = \mathbb{E}[X_i^2] - \mu^2 = \sigma ^2
    \end{equation*}
    Also, we have
    \begin{align*}
        \mathrm{Var}[\bar{X}] &= \frac{\sigma^2}{n} \\
        \mathbb{E}[\bar{X}^2] &= \mathrm{Var}[\bar{X}] + \mathbb{E}[\bar{X}]^2 \\
        &= \frac{\sigma^2}{n} + \mu^2
    \end{align*}
    \begin{align*}
        \mathbb{E}[S^2] &= \mathbb{E}\bigg[\frac{1}{n-1}\sum_{i=1}^{n} (X_i - \bar{X})^2 \bigg]\\
        &= \frac{1}{n-1} \mathbb{E}\bigg[\sum_{i=1}^{n} \bigg( X_i^2 - 2X_i \bar{X} + \bar{X}^2\bigg) \bigg] \\
        &= \frac{1}{n-1} \mathbb{E}\bigg[\sum_{i=1}^{n}  X_i^2  - n \bar{X^2} \bigg] \\
        &= \frac{1}{n-1} \bigg( n(\sigma^2 +\mu^2) - n (\frac{\sigma^2}{n} + \mu^2) \bigg) \\
        &= \sigma^2
    \end{align*}
    \item[(c)] Since $X_i$-s have i.i.d. normal/Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$
    \begin{align*}
        \mathrm{Cov}[\bar{X},  X_i - \bar{X}] &=  \mathrm{Cov}[\bar{X},  X_i ] - \mathrm{Var}[\bar{X}]\\
        &= \frac{1}{n}\mathrm{Cov}[X_i + \sum_{j\neq i}^{}  X_j X_i] - \frac{\sigma^2}{n} \\
        &= \frac{1}{n}\mathrm{Var}[X_i] - \frac{\sigma^2}{n} \\
        &= 0
    \end{align*}
    therefore, $\bar{X}$ is independent of $X_i-\bar{X}$.
    \item[(d)] Since sample variance
    \begin{equation*}
        S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2
    \end{equation*}
    is a function of $X_i-\bar{X}$, which is independent of $\bar{X}$,
    sample mean is also independent of sample variance.
\end{enumerate}

\section*{P2}
Assume that $\bar{y} = \bar{x}=0$,
then we have,
\begin{equation*}
    R^2 = \frac{\sum_{i=1}^{n}( \hat{y_i})^2}{\sum_{i=1}^{n} y_i^2} = \frac{1}{\sum_{i=1}^{n} y_i^2} \sum_{i=1}^{n} \bigg( \frac{\sum_{i=1}^{n}x_i y_i}{\sum_{i=1}^{n}x_i^2}x_i \bigg)^2 = \frac{1}{\sum_{i=1}^{n} y_i^2}  \bigg( \frac{\sum_{i=1}^{n}x_i y_i}{\sum_{i=1}^{n}x_i^2} \bigg)^2 \sum_{i=1}^{n} x_i^2 = \frac{(\sum_{i=1}^{n} x_i y_i)^2}{\sum_{i=1}^{n} x_i^2 \sum_{i=1}^{n} y_i^2} 
\end{equation*}
and
\begin{equation*}
    r^2 = \frac{(\sum_{i=1}^{n} x_i y_i)^2}{\sum_{i=1}^{n} x_i^2 \sum_{i=1}^{n} y_i^2} 
\end{equation*}
Hence, $R^2$ = $r^2$

\section*{P3}
\subsection*{simple linear regression }
```{r}
set.seed(1)
x = rnorm(100)
eps = rnorm(100, mean = 0, sd = 0.25)
y = -1 + 0.5*x + eps
```
The length of vector $y$ is 100. In this linear model $\beta_0 = -1$ and $\beta_1 = 0.5$. 
```{r fig1, fig.height = 2, fig.width = 3, fig.align = "center", fig.cap="\\label{fig:figs}scatter plot of X and Y"}
library(ggplot2)
scatter_fig <- ggplot(,aes(x=x, y=y)) + geom_point() 
scatter_fig
```

From the scatter plot, we can see a rough line with slop 0.5. The data points in both x-direction and y-direction are centered in the middle.
```{r}
simple_lm <-lm(y~x)
summary(simple_lm)
```
From the summary, we can clear see that $\hat{\beta_0}$ = `r simple_lm$coefficients[1]` and $\hat{\beta_1}$ = `r simple_lm$coefficients[2]`. Both of them are only close to its true value. Notice that I used legend inside `ggplot2` instead of `legend()`.
```{r fig2, fig.height = 2, fig.width = 5, fig.align = "center", fig.cap="scatter plot of X and Y with regression lines"}
scatter_fig2 <- scatter_fig + geom_abline( aes(intercept = simple_lm$coefficients[1], 
                                               slope = simple_lm$coefficients[2],color = "red"), 
                 linetype="dashed", size=1.5) +
  geom_smooth(method = "loess", aes(color = "blue"), 
                 linetype="dashed", size=1.5, se = FALSE) +
  scale_colour_manual(name='Lines', labels = c("population regression", "least squares"), 
                      values=c("blue", "red"))
scatter_fig2
```

```{r}
quadratic_lm <- lm(y ~ x + I(x^2))
summary(quadratic_lm)
```

There is no evidence to show that quadratic term improves the model fit. Because `Pr(>|t|)` = 0.164, which is larger than 0.05.

\subsection*{simple linear regression with less noise}

```{r}
x = rnorm(100)
eps = rnorm(100, mean = 0, sd = 0.1)
y = -1 + 0.5*x + eps
```
The length of vector $y$ is 100. In this linear model $\beta_0 = -1$ and $\beta_1 = 0.5$. Here  let $X \sim \mathcal{N}(0, 0.1)$ to reduce noise.
```{r fig3, fig.height = 2, fig.width = 3, fig.align = "center", fig.cap="\\label{fig:figs}scatter plot of X and Y"}
library(ggplot2)
scatter_fig <- ggplot(,aes(x=x, y=y)) + geom_point() 
scatter_fig
```

From the scatter plot, we can see a clear line with slop 0.5. The data points in both x-direction and y-direction are centered in the middle.
```{r}
simple_less_lm <-lm(y~x)
summary(simple_less_lm)
```
From the summary, we can clear see that $\hat{\beta_0}$ = `r simple_less_lm$coefficients[1]` and $\hat{\beta_1}$ = `r simple_less_lm$coefficients[2]`. Both of them are very close to its true value. The significance is slightly more important than previous model. Notice that I used legend inside `ggplot2` instead of `legend()`. Compared with previous figure, the population regression line overlaps largely least squares line.
```{r fig4, fig.height = 2, fig.width = 5, fig.align = "center", fig.cap="scatter plot of X and Y with regression lines"}
scatter_fig2 <- scatter_fig + geom_abline( aes(intercept = simple_less_lm$coefficients[1], 
                                               slope = simple_less_lm$coefficients[2],color = "red"), 
                 linetype="dashed", size=1.5) +
  geom_smooth(method = "loess", aes(color = "blue"), 
                 linetype="dashed", size=1.5, se = FALSE) +
  scale_colour_manual(name='Lines', labels = c("population regression", "least squares"), 
                      values=c("blue", "red"))
scatter_fig2
```

\subsection*{simple linear regression with more noise}
```{r}
x = rnorm(100)
eps = rnorm(100, mean = 0, sd = 1)
y = -1 + 0.5*x + eps
```
The length of vector $y$ is 100. In this linear model $\beta_0 = -1$ and $\beta_1 = 0.5$. Here  let $X \sim \mathcal{N}(0, 1)$ to add more noise.
```{r fig5, fig.height = 2, fig.width = 3, fig.align = "center", fig.cap="\\label{fig:figs}scatter plot of X and Y"}
library(ggplot2)
scatter_fig <- ggplot(,aes(x=x, y=y)) + geom_point() 
scatter_fig
```

From the scatter plot, we can only see a bunch of data points. The data points in both x-direction and y-direction are centered in the middle.
```{r}
simple_more_lm <-lm(y~x)
summary(simple_more_lm)
```
From the summary, we can clear see that $\hat{\beta_0}$ = `r simple_more_lm$coefficients[1]` and $\hat{\beta_1}$ = `r simple_more_lm$coefficients[2]`. Both of them are far away from its true value. The significance is much worse than previous model. Notice that I used legend inside `ggplot2` instead of `legend()`. Compared with previous figure, the population regression line has a large difference with least squares line.
```{r fig6, fig.height = 2, fig.width = 5, fig.align = "center", fig.cap="scatter plot of X and Y with regression lines"}
scatter_fig2 <- scatter_fig + geom_abline( aes(intercept = simple_more_lm$coefficients[1], 
                                               slope = simple_more_lm$coefficients[2],color = "red"), 
                 linetype="dashed", size=1.5) +
  geom_smooth(method = "loess", aes(color = "blue"), 
                 linetype="dashed", size=1.5, se = FALSE) +
  scale_colour_manual(name='Lines', labels = c("population regression", "least squares"), 
                      values=c("blue", "red"))
scatter_fig2
```
```{r}
# "Confidence Interval for original data set"
confint(simple_lm , c("(Intercept)", "x"), level = 0.95)

# "Confidence Interval for less noisy data set"
confint(simple_less_lm , c("(Intercept)", "x"), level = 0.95)

# "Confidence Interval for noisier data set"
confint(simple_more_lm , c("(Intercept)", "x"), level = 0.95)
```
Here are the confidence interval for the original data set, the noisier data set, and the less noisy data set. `(intercept)` represents $\hat{\beta_0}$ and `x` represents $\hat{\beta_1}$. While the confidence interval for original data set and less noisy data set are very close to each other, confidence interval for noiser data set are much wider due more noise.

\section*{P4}

```{r}
adv.df = read.csv("Advertising.csv", header=T, na.string=",")
TV.lm <- lm(sales ~ TV, data = adv.df)
radio.lm <- lm(sales ~ radio, data = adv.df)
newspaper.lm <- lm(sales ~ newspaper, data = adv.df)

# "Confidence Interval for sales ~ TV"
confint(TV.lm , c("(Intercept)", "TV"), level = 0.92)

# "Confidence Interval for sales ~ radio"
confint(radio.lm , c("(Intercept)", "radio"), level = 0.92)

# "Confidence Interval for sales ~ newspaper"
confint(newspaper.lm , c("(Intercept)", "newspaper"), level = 0.92)
```

Here are 92% confidence intervals for $\hat{\beta_0}$ and $\hat{\beta_1}$ for three linear regressions of `sales` onto `newspaper`, `TV` and `radio`. Three scatterplots with confidence interval are shown below.

```{r TV_confint, fig.height = 4, fig.width = 5, fig.align = "center", fig.cap="confidence interval for \\texttt{sales} $\\sim$ \\texttt{TV}"}
plot(adv.df$TV, adv.df$sales, xlab="TV", ylab="sales", pch=20)
new_TV <- seq(min(adv.df$TV), max(adv.df$TV), length.out=100)
preds <- predict(TV.lm, newdata = data.frame(TV=new_TV), 
                 interval = 'confidence', level = 0.92)
polygon(c(rev(new_TV), new_TV), c(rev(preds[ ,3]), preds[ ,2]), col = 'grey80', border = NA)
abline(TV.lm, col = 'blue')
# intervals
lines(new_TV, preds[ ,3], lty = 'dashed', col = 'red')
lines(new_TV, preds[ ,2], lty = 'dashed', col = 'red')
```

```{r newspaper_confint, fig.height = 4, fig.width = 5, fig.align = "center", fig.cap="confidence interval for \\texttt{sales} $\\sim$ \\texttt{newspaper}"}
plot(adv.df$newspaper, adv.df$sales, xlab="newspaper", ylab="sales", pch=20)
new_newspaper <- seq(min(adv.df$newspaper), max(adv.df$newspaper), length.out=100)
preds <- predict(newspaper.lm, newdata = data.frame(newspaper=new_newspaper), 
                 interval = 'confidence', level = 0.92)
polygon(c(rev(new_newspaper), new_newspaper), c(rev(preds[ ,3]), preds[ ,2]), col = 'grey80', border = NA)
abline(newspaper.lm, col = 'blue')
# intervals
lines(new_newspaper, preds[ ,3], lty = 'dashed', col = 'red')
lines(new_newspaper, preds[ ,2], lty = 'dashed', col = 'red')
```

```{r radio_confint, fig.height = 4, fig.width = 5, fig.align = "center", fig.cap="confidence interval for \\texttt{sales} $\\sim$ \\texttt{radio}"}
plot(adv.df$radio, adv.df$sales, xlab="radio", ylab="sales", pch=20)
new_radio <- seq(min(adv.df$radio), max(adv.df$radio), length.out=100)
preds <- predict(radio.lm, newdata = data.frame(radio=new_radio), 
                 interval = 'confidence', level = 0.95)
polygon(c(rev(new_radio), new_radio), c(rev(preds[ ,3]), preds[ ,2]), col = 'grey80', border = NA)
abline(radio.lm, col = 'blue')
# intervals
lines(new_radio, preds[ ,3], lty = 'dashed', col = 'red')
lines(new_radio, preds[ ,2], lty = 'dashed', col = 'red')
```


\section*{P5}

```{r}
Auto.df <- read.csv("Auto.csv", header=T, na.strings="?")
Auto.df <- na.omit(Auto.df)
```

```{r scatter_mat, fig.align = "center", fig.cap="scatter matrix for Auto data set"}
pairs(Auto.df[,1:9])
```

Here is the correlation matrix between the variables exclude `name`
```{r}
cor(Auto.df[,1:8])
```

```{r}
Auto.lm <- lm(mpg ~ cylinders + displacement + horsepower + 
                weight + acceleration + year + origin,  data = Auto.df)
summary(Auto.lm)
```

Since the `Adjusted R-squared` term is 0.8182, the linear model could fit the relationship between predictors and response. From the summary of linear regression model using all numeric variables, we can clearly see that it is evident that `cylinders`, `horsepower` and `acceleration` are not related with `mpg`. The other variables are related with response. According to coefficients in summary, predictors `displacement`, `year` and `origin` have a positive correlation with `mpg`, while `weight` have a negative correlation with `mpg`. This result makes common sense, since as the car becomes heavier, mpg will decrease. And as time flies, the car also becomes more powerful and mpg also increases.

Since some of predictors don't have a statistically significant relationship to the response. Therefore, I will try a few different transformations of the variables.
```{r}
Auto1.lm <- lm(mpg ~ cylinders + displacement + sqrt(horsepower) + 
                 weight + acceleration + year + origin,  data = Auto.df)
summary(Auto1.lm)
```

Here I used `sqrt(horsepower)` instead of original `horsepower`. From the summary, we can see that the new variable has a significant relationship to the response.

```{r}
Auto2.lm <- lm(mpg ~ cylinders + displacement + sqrt(horsepower) +
                 weight + log(acceleration) + year + origin,  data = Auto.df)
summary(Auto2.lm)
```
And I also switch to use `log(acceleration)` instead of `acceleration`, the significance of that term is a little improved. But overall `Adjusted R-squared` only improved little.

```{r}
Auto3.lm <- lm(log(mpg) ~ displacement + sqrt(horsepower) + weight +
                 log(acceleration) + year + origin,  data = Auto.df)
summary(Auto3.lm)
```

Also, I tried to use transformation on the response. I used `log(mpg)` to subsitute `mpg`.  Athough the `displacement` term becomes not significant again, overall `Adjusted R-squared` is improved.

\section*{P6}
\begin{align*}
    \hat{\beta_1} &= \frac{n \sum x_i y_i - \sum x_i \sum y_i}{n \sum x_i^2 - (\sum x_i)^2} \\
    &= \frac{20 \cdot 216.6 - 8.552 \cdot 398.2 }{20 \cdot 5.196 - (8.552)^2} \\
    &= 30.10
\end{align*}

\begin{align*}
    \hat{\beta_0} &= \bar{y} - \hat{\beta_1} \bar{x} \\
    &= 7.04
\end{align*}

\begin{align*}
    \hat{\sigma} ^2 &= \frac{1}{n-2} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \\
    &= \frac{1}{n-2} \sum_{i=1}^{n} y_i^2 + \hat{\beta_0}^2 + \hat{\beta_1}^2 x_i ^2 - 2 \hat{\beta_0}y_i - 2 \hat{\beta_1}x_i y_i + 2 \hat{\beta_0} \hat{\beta_1} x_1 \\
    &= \frac{1}{18} (9356 + 20 \cdot 7.04^2 + 30.10^2 \cdot 5.196 - 2 \cdot 7.04 \cdot 398.2 - 2 \cdot 30.10 \cdot 216.6 + 2 \cdot 7.04 \cdot 30.10 \cdot 8.552) \\
    &= 1.8023
\end{align*}

\begin{align*}
    \hat{y}_{x = 0.5} = \hat{\beta_0} + \hat{\beta_1} x = 22.09
\end{align*}

\begin{align*}
    R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} =  1- \frac{32.44}{1427.84} = 0.977
\end{align*}

\section*{P7}
Null hypothesis:
\begin{equation*}
     \mathcal{H}_0 = \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = 0
\end{equation*}
Here we have
\begin{equation*}
     k = 6 \quad n = 45
\end{equation*}
\begin{equation*}
    \mathrm{TSS} = 11.62 \quad \mathrm{RSS} = 8.95
\end{equation*}
\begin{equation*}
    \mathcal{F}_{k, n-k-1} = \frac{(\mathrm{TSS}-\mathrm{RSS})/k}{\mathrm{RSS}/(n-k-1)} = 1.889
\end{equation*}
```{r}
# p-value
1-pf(1.889385,6,45-6-1)
```

Also, at $\alpha = 0.05$, we have
\begin{equation*}
    f_{\alpha, k, n-k-1} = 2.349027
\end{equation*}
Therefore, $\mathcal{F}_{k, n-k-1} < f_{\alpha, k, n-k-1}$, then $\mathcal{H}_0$ cannot be rejected, which means there is  reason to believe that the regression is
not significant.

